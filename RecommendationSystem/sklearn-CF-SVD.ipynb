{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(dataFile, test_size):\n",
    "    names = ['user_id','item_id','rating','timestamp']\n",
    "    df = pd.read_csv(dataFile, sep='\\t',names=names)\n",
    "    \n",
    "    n_users = df.user_id.unique().shape[0]\n",
    "    n_items = df.item_id.unique().shape[0]\n",
    "    print('Number of users = %d \\n Number of movies = %d' % (n_users, n_items))\n",
    "    \n",
    "    train_data, test_data = train_test_split(df, test_size=test_size)\n",
    "    print('数据量：', len(train_data), len(test_data))\n",
    "    return df, n_users, n_items, train_data, test_data\n",
    "\n",
    "def calc_similarity(n_users, n_items, train_data, test_data):\n",
    "    train_data_matrix = np.zeros((n_users, n_items))\n",
    "    for line in train_data.itertuples():\n",
    "        train_data_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "    test_data_matrix = np.zeros((n_users, n_items))\n",
    "    for line in test_data.itertuples():\n",
    "        test_data_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "    \n",
    "    print('1:', np.shape(train_data_matrix))\n",
    "    print('2:', np.shape(test_data_matrix.T))\n",
    "    \n",
    "    user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "    item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')\n",
    "    \n",
    "    print('开始统计流行item的数量 ...')\n",
    "    item_popular = {}\n",
    "    for i_index in range(n_items):\n",
    "        if np.sum(train_data_matrix[:,i_index]) != 0:\n",
    "            item_popular[i_index] = np.sum(train_data_matrix[:,i_index] != 0)\n",
    "    item_count = len(item_popular)\n",
    "    print('总共流行item数量 = %d' % item_count)\n",
    "    \n",
    "    return train_data_matrix, test_data_matrix, user_similarity, item_similarity, item_popular\n",
    "\n",
    "def predict(rating, similarity, type='user'):\n",
    "    print(type)\n",
    "    print('rating = ', np.shape(rating))\n",
    "    print('similarity = ', np.shape(similarity))\n",
    "    if type == 'user':\n",
    "        mean_user_rating = rating.mean(axis=1)\n",
    "        rating_diff = (rating - mean_user_rating[:,np.newaxis])\n",
    "        pred = mean_user_rating[:,np.newaxis] + similarity.dot(rating_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = rating.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred\n",
    "\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten()\n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return math.sqrt(mean_squared_error(prediction, ground_truth))\n",
    "\n",
    "def evaluate(prediction, item_popular, name):\n",
    "    hit = 0\n",
    "    rec_count = 0\n",
    "    test_count = 0\n",
    "    popular_sum = 0\n",
    "    all_rec_items = set()\n",
    "    for u_index in range(n_users):\n",
    "        items = np.where(train_data_matrix[u_index,:] == 0)[0]\n",
    "        pre_items = sorted(dict(zip(items, prediction[u_index, items])).items(),\n",
    "                          key=itemgetter(1),\n",
    "                          reverse=True)[:20]\n",
    "        test_items = np.where(test_data_matrix[u_index,:] != 0)[0]\n",
    "        \n",
    "        for item, _ in pre_items:\n",
    "            if item in test_items:\n",
    "                hit += 1\n",
    "            all_rec_items.add(item)\n",
    "            \n",
    "            if item in item_popular:\n",
    "                popular_sum += math.log(1 + item_popular[item])\n",
    "                \n",
    "        rec_count += len(pre_items)\n",
    "        test_count += len(test_items)\n",
    "        \n",
    "    precision = hit / (1.0 * rec_count)\n",
    "    recall = hit / (1.0 * test_count)\n",
    "    coverage = len(all_rec_items) / (1.0 * len(item_popular))\n",
    "    popularity = popular_sum / (1.0 * rec_count)\n",
    "    print('%s: precision = %.4f \\t recall = %.4f \\t coverage = %.4f \\t popularity = %.4f' % (name, precision, recall, coverage, popularity))\n",
    "    \n",
    "def recommend(u_index, prediction):\n",
    "    items = np.where(train_data_matrix[u_index,:] == 0)[0]\n",
    "    pre_items = sorted(dict(zip(items, prediction[u_index, items])).items(),\n",
    "                      key=itemgetter(1),\n",
    "                      reverse=True)[:10]\n",
    "    test_items = np.where(test_data_matrix[u_index,:] != 0)[0]\n",
    "    \n",
    "    print('原始结果：', test_items)\n",
    "    print('推荐结果：', [key for key, value in pre_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = 'u.data'\n",
    "df, n_users, n_items, train_data, test_data = splitData(dataFile, \n",
    "                                                        test_size=0.25)\n",
    "\n",
    "train_data_matrix, test_data_matrix, user_similarity, item_similarity, item_popular = calc_similarity(n_users,\n",
    "                                                                                                     n_items,\n",
    "                                                                                                     train_data,\n",
    "                                                                                                     test_data)\n",
    "item_prediction = predict(train_data_matrix,\n",
    "                         item_similarity,\n",
    "                         type='item')\n",
    "user_prediction = predict(train_data_matrix,\n",
    "                         user_similarity,\n",
    "                         type='user')\n",
    "\n",
    "print('Item Based CF RMSE: ', rmse(item_prediction, test_data_matrix))\n",
    "print('User Based CF RMSE: ', rmse(user_prediction, test_data_matrix))\n",
    "\n",
    "sparsity = round(1.0 * len(df) / float(n_users * n_items), 3)\n",
    "print('The sparsity level of MovieLen100K is ' + str(sparsity * 100) + '%')\n",
    "\n",
    "u, s, vt = svds(train_data_matrix, k=15)\n",
    "s_diag_matrix = np.diag(s)\n",
    "svd_prediction = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print('svd-shape: ', np.shape(svd_prediction))\n",
    "print('Model based CF RMSE: ', rmse(svd_prediction, test_data_matrix))\n",
    "\n",
    "evaluate(item_prediction, item_popular, 'item')\n",
    "evaluate(user_prediction, item_popular, 'user')\n",
    "evaluate(svd_prediction, item_popular, 'svd')\n",
    "\n",
    "recommend(1, svd_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
